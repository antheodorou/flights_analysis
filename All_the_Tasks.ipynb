{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "\n",
    "# International Flights\n",
    "\n",
    "##  Big Data Systems and Architectures\n",
    "\n",
    "## Spark Assignment\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> Anastasios Theodorou, Student <br />\n",
    "> Master of Science in Business Analytics <br />\n",
    "> Department of Management Science and Technology <br />\n",
    "> Athens University of Economics and Business <br />\n",
    "> AM: p2822007\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.evaluation\n",
    "from pyspark.ml.feature import IndexToString, MinMaxScaler, StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName('appName').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7422037"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the whole dataset\n",
    "flight_data = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"671009038_T_ONTIME_REPORTING.csv\")\n",
    "flight_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+------+----------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+\n",
      "|   FL_DATE|TAIL_NUM|CARRIER|ORIGIN|ORIGIN_CITY_NAME|DEST|    DEST_CITY_NAME|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|_c19|\n",
      "+----------+--------+-------+------+----------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+\n",
      "|2019-01-01|  N8974C|     9E|   AVL|   Asheville, NC| ATL|       Atlanta, GA|    1658|     -7.0|    1758|    -22.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N922XJ|     9E|   JFK|    New York, NY| RDU|Raleigh/Durham, NC|    1122|     -8.0|    1255|    -29.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "|2019-01-01|  N326PQ|     9E|   CLE|   Cleveland, OH| DTW|       Detroit, MI|    1334|     -7.0|    1417|    -31.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|\n",
      "+----------+--------+-------+------+----------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data.createOrReplaceTempView(\"flight_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|DEP_DELAY|ARR_DELAY|\n",
      "+---------+---------+\n",
      "|     -7.0|    -22.0|\n",
      "|     -8.0|    -29.0|\n",
      "|     -7.0|    -31.0|\n",
      "|     -1.0|     -8.0|\n",
      "|     -3.0|    -17.0|\n",
      "|      0.0|     10.0|\n",
      "|     -5.0|    -16.0|\n",
      "|    -10.0|    -29.0|\n",
      "|     -4.0|    -18.0|\n",
      "|     -4.0|     -6.0|\n",
      "|     -5.0|    -20.0|\n",
      "|     -9.0|     -4.0|\n",
      "|     -6.0|    -18.0|\n",
      "|     -6.0|    -18.0|\n",
      "|     -1.0|    -14.0|\n",
      "|     -5.0|      3.0|\n",
      "|    124.0|    109.0|\n",
      "|     -4.0|    -10.0|\n",
      "|     -4.0|     -7.0|\n",
      "|     -1.0|     -4.0|\n",
      "+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#replace the null values with 0 number\n",
    "delay = spark.sql(\"\"\"\n",
    "SELECT ifnull(DEP_DELAY, '0') as DEP_DELAY, ifnull(ARR_DELAY, '0') as ARR_DELAY\n",
    "FROM flight_data \n",
    "\"\"\")\n",
    "delay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay.createOrReplaceTempView(\"delay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|  departures_delay|   arrivals_delay|\n",
      "+------------------+-----------------+\n",
      "|10.923267333861132|5.414849168270909|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delay = spark.sql(\"\"\"\n",
    "SELECT avg(DEP_DELAY) as departures_delay, avg(ARR_DELAY) as arrivals_delay\n",
    "FROM flight_data\n",
    "\"\"\")\n",
    "delay.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+------+----------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+----------+----------+\n",
      "|   FL_DATE|TAIL_NUM|CARRIER|ORIGIN|ORIGIN_CITY_NAME|DEST|    DEST_CITY_NAME|DEP_TIME|DEP_DELAY|ARR_TIME|ARR_DELAY|CANCELLED|CANCELLATION_CODE|DIVERTED|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|_c19|DEP_DELAY1|ARR_DELAY1|\n",
      "+----------+--------+-------+------+----------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+----------+----------+\n",
      "|2019-01-01|  N8974C|     9E|   AVL|   Asheville, NC| ATL|       Atlanta, GA|    1658|     -7.0|    1758|    -22.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|      -7.0|     -22.0|\n",
      "|2019-01-01|  N922XJ|     9E|   JFK|    New York, NY| RDU|Raleigh/Durham, NC|    1122|     -8.0|    1255|    -29.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|      -8.0|     -29.0|\n",
      "|2019-01-01|  N326PQ|     9E|   CLE|   Cleveland, OH| DTW|       Detroit, MI|    1334|     -7.0|    1417|    -31.0|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|      -7.0|     -31.0|\n",
      "+----------+--------+-------+------+----------------+----+------------------+--------+---------+--------+---------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convert the NULL values with 0 number\n",
    "flight_data = spark.sql(\"\"\"\n",
    "SELECT *, ifnull(DEP_DELAY, '0') as DEP_DELAY1, ifnull(ARR_DELAY, '0') as ARR_DELAY1\n",
    "FROM flight_data \n",
    "\"\"\")\n",
    "flight_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the previous columns referring to the delays\n",
    "flight_data = flight_data.drop(\"DEP_DELAY\", \"ARR_DELAY\")\n",
    "flight_data.createOrReplaceTempView(\"flight_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+------+----------------+----+------------------+--------+--------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+----------+----------+\n",
      "|   FL_DATE|TAIL_NUM|CARRIER|ORIGIN|ORIGIN_CITY_NAME|DEST|    DEST_CITY_NAME|DEP_TIME|ARR_TIME|CANCELLED|CANCELLATION_CODE|DIVERTED|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|_c19|DEP_DELAY1|ARR_DELAY1|\n",
      "+----------+--------+-------+------+----------------+----+------------------+--------+--------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+----------+----------+\n",
      "|2019-01-01|  N8974C|     9E|   AVL|   Asheville, NC| ATL|       Atlanta, GA|    1658|    1758|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|      -7.0|     -22.0|\n",
      "|2019-01-01|  N922XJ|     9E|   JFK|    New York, NY| RDU|Raleigh/Durham, NC|    1122|    1255|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|      -8.0|     -29.0|\n",
      "|2019-01-01|  N326PQ|     9E|   CLE|   Cleveland, OH| DTW|       Detroit, MI|    1334|    1417|      0.0|             null|     0.0|         null|         null|     null|          null|               null|null|      -7.0|     -31.0|\n",
      "+----------+--------+-------+------+----------------+----+------------------+--------+--------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----+----------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_data.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|airports|flight_numbers|\n",
      "+--------+--------------+\n",
      "|     DCA|        139388|\n",
      "|     IAH|        179688|\n",
      "|     LGA|        171665|\n",
      "|     BOS|        150564|\n",
      "|     EWR|        136081|\n",
      "|     LAS|        164020|\n",
      "|     DEN|        252026|\n",
      "|     SEA|        142857|\n",
      "|     CLT|        235496|\n",
      "|     BNA|         82654|\n",
      "|     MIA|         89214|\n",
      "|     TPA|         76599|\n",
      "|     BWI|        104652|\n",
      "|     PHX|        175328|\n",
      "|     DFW|        304344|\n",
      "|     SFO|        170918|\n",
      "|     ATL|        395009|\n",
      "|     FLL|         98409|\n",
      "|     ORD|        339606|\n",
      "|     MDW|         83763|\n",
      "+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create the dataset of the airports (in the DEST column the airports were\n",
    "#the same as in the origin column)\n",
    "flights = spark.sql(\"\"\"\n",
    "SELECT ORIGIN AS airports, count(ORIGIN) as flight_numbers\n",
    "FROM flight_data \n",
    "GROUP BY ORIGIN\n",
    "HAVING count(ORIGIN) > (SELECT count(ORIGIN)*0.01 FROM flight_data)\n",
    "\"\"\")\n",
    "flights.show() #the number of flights for each airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(flights.count())#the size of the unique airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|airways|flight_numbers|\n",
      "+-------+--------------+\n",
      "|     UA|        625910|\n",
      "|     NK|        204845|\n",
      "|     AA|        946776|\n",
      "|     EV|        134683|\n",
      "|     B6|        297411|\n",
      "|     DL|        991986|\n",
      "|     OO|        836445|\n",
      "|     F9|        135543|\n",
      "|     YV|        227888|\n",
      "|     MQ|        327007|\n",
      "|     OH|        289304|\n",
      "|     HA|         83891|\n",
      "|     G4|        105305|\n",
      "|     YX|        329149|\n",
      "|     AS|        264816|\n",
      "|     WN|       1363946|\n",
      "|     9E|        257132|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create the dataset of the airways\n",
    "airways = spark.sql(\"\"\"\n",
    "SELECT CARRIER AS airways, count(CARRIER) as flight_numbers\n",
    "FROM flight_data \n",
    "GROUP BY CARRIER\n",
    "HAVING count(CARRIER) > (SELECT count(CARRIER)*0.01 FROM flight_data)\n",
    "\"\"\")\n",
    "airways.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "print(airways.count())#the size of the unique airways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make those datasets views in order to use them in the below queries\n",
    "flights.createOrReplaceTempView(\"flights\")\n",
    "airways.createOrReplaceTempView(\"airways\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|Airports|Departures_Delay|\n",
      "+--------+----------------+\n",
      "|     EWR|          17.895|\n",
      "|     LGA|          14.579|\n",
      "|     ORD|          14.249|\n",
      "|     BOS|          13.804|\n",
      "|     FLL|           13.76|\n",
      "|     MCO|          13.523|\n",
      "|     DEN|          13.332|\n",
      "|     SFO|          13.301|\n",
      "|     MDW|          13.092|\n",
      "|     DFW|          12.756|\n",
      "|     JFK|          12.466|\n",
      "|     IAH|          12.399|\n",
      "|     BNA|          10.909|\n",
      "|     BWI|          10.764|\n",
      "|     MIA|          10.573|\n",
      "|     CLT|          10.564|\n",
      "|     TPA|          10.517|\n",
      "|     DCA|          10.454|\n",
      "|     LAS|          10.307|\n",
      "|     PHL|          10.172|\n",
      "+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we want the avg for the departures delays for every airport (origin)\n",
    "avg_airports = spark.sql(\"\"\"\n",
    "SELECT fd.ORIGIN as Airports, round(avg(fd.DEP_DELAY1), 3) as Departures_Delay\n",
    "FROM flight_data as fd, flights as f\n",
    "WHERE fd.ORIGIN = f.airports\n",
    "GROUP BY fd.ORIGIN\n",
    "ORDER BY avg(fd.DEP_DELAY1) DESC\"\"\")\n",
    "avg_airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_airports.toPandas().to_csv('task2-ap-avg.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+\n",
      "|NUM_ROW|ORIGIN|DEP_DELAY|\n",
      "+-------+------+---------+\n",
      "|      1|   DCA|     -1.0|\n",
      "|      2|   DCA|     -1.0|\n",
      "|      3|   DCA|     -1.0|\n",
      "|      4|   DCA|     -1.0|\n",
      "|      5|   DCA|     -1.0|\n",
      "|      6|   DCA|     -1.0|\n",
      "|      7|   DCA|     -1.0|\n",
      "|      8|   DCA|     -1.0|\n",
      "|      9|   DCA|     -1.0|\n",
      "|     10|   DCA|     -1.0|\n",
      "|     11|   DCA|     -1.0|\n",
      "|     12|   DCA|     -1.0|\n",
      "|     13|   DCA|     -1.0|\n",
      "|     14|   DCA|     -1.0|\n",
      "|     15|   DCA|     -1.0|\n",
      "|     16|   DCA|     -1.0|\n",
      "|     17|   DCA|     -1.0|\n",
      "|     18|   DCA|     -1.0|\n",
      "|     19|   DCA|     -1.0|\n",
      "|     20|   DCA|     -1.0|\n",
      "+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#place an iterator\n",
    "c_airports = spark.sql(\"\"\"\n",
    "SELECT ROW_NUMBER() OVER(PARTITION BY ORIGIN \n",
    "                            ORDER BY DEP_DELAY1 ASC) AS NUM_ROW,\n",
    "        ORIGIN, DEP_DELAY1 AS DEP_DELAY\n",
    "FROM flight_data, flights as f\n",
    "WHERE ORIGIN = f.airports\n",
    "\"\"\")\n",
    "c_airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|ORIGIN|   COR|\n",
      "+------+------+\n",
      "|   DCA|139388|\n",
      "|   IAH|179688|\n",
      "|   LGA|171665|\n",
      "|   BOS|150564|\n",
      "|   EWR|136081|\n",
      "|   LAS|164020|\n",
      "|   DEN|252026|\n",
      "|   SEA|142857|\n",
      "|   BNA| 82654|\n",
      "|   CLT|235496|\n",
      "|   MIA| 89214|\n",
      "|   BWI|104652|\n",
      "|   TPA| 76599|\n",
      "|   PHX|175328|\n",
      "|   DFW|304344|\n",
      "|   SFO|170918|\n",
      "|   ATL|395009|\n",
      "|   FLL| 98409|\n",
      "|   ORD|339606|\n",
      "|   MDW| 83763|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#save the length of the airport's flights\n",
    "count_airports = spark.sql(\"\"\"\n",
    "SELECT ORIGIN, count(ORIGIN) as COR\n",
    "FROM flight_data, flights as f\n",
    "WHERE ORIGIN = f.airports\n",
    "GROUP BY ORIGIN\n",
    "\"\"\")\n",
    "count_airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_airports.createOrReplaceTempView(\"count_airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+\n",
      "|ORIGIN|   COR|     P|    P1|\n",
      "+------+------+------+------+\n",
      "|   DCA|139388| 69694| 69695|\n",
      "|   IAH|179688| 89844| 89845|\n",
      "|   LGA|171665| 85833|     0|\n",
      "|   BOS|150564| 75282| 75283|\n",
      "|   EWR|136081| 68041|     0|\n",
      "|   LAS|164020| 82010| 82011|\n",
      "|   DEN|252026|126013|126014|\n",
      "|   SEA|142857| 71429|     0|\n",
      "|   BNA| 82654| 41327| 41328|\n",
      "|   CLT|235496|117748|117749|\n",
      "|   MIA| 89214| 44607| 44608|\n",
      "|   BWI|104652| 52326| 52327|\n",
      "|   TPA| 76599| 38300|     0|\n",
      "|   PHX|175328| 87664| 87665|\n",
      "|   DFW|304344|152172|152173|\n",
      "|   SFO|170918| 85459| 85460|\n",
      "|   ATL|395009|197505|     0|\n",
      "|   FLL| 98409| 49205|     0|\n",
      "|   ORD|339606|169803|169804|\n",
      "|   MDW| 83763| 41882|     0|\n",
      "+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the lines where the medians are\n",
    "count_airports = spark.sql(\"\"\"\n",
    "SELECT ORIGIN, COR, CAST(IF(MOD(COR, 2)=0, COR/2, (COR+1)/2) AS int) AS P,\n",
    "        CAST(IF(MOD(COR, 2)=0, COR/2+1, 0) AS int) AS P1\n",
    "FROM count_airports\n",
    "\"\"\")\n",
    "count_airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_airports.createOrReplaceTempView(\"c_airports\")\n",
    "count_airports.createOrReplaceTempView(\"count_airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|AIRPORT|DELAY1|\n",
      "+-------+------+\n",
      "|    SAN|  -7.0|\n",
      "|    MDW|   1.0|\n",
      "|    SFO|  -8.0|\n",
      "|    CLT|  -7.0|\n",
      "|    MIA|  -7.0|\n",
      "|    MCO|  -8.0|\n",
      "|    PHX|  -8.0|\n",
      "|    FLL|  -8.0|\n",
      "|    ATL|  -6.0|\n",
      "|    BOS|  -7.0|\n",
      "|    SLC|  -6.0|\n",
      "|    BNA|  -8.0|\n",
      "|    DCA|  -7.0|\n",
      "|    DFW|  -7.0|\n",
      "|    SEA|  -7.0|\n",
      "|    EWR|  -8.0|\n",
      "|    PHL|  -7.0|\n",
      "|    JFK|  -7.0|\n",
      "|    DTW|  -6.0|\n",
      "|    MSP|  -5.0|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#match to each P value the recording row of the delay\n",
    "p_set = spark.sql(\"\"\"\n",
    "SELECT air.ORIGIN as AIRPORT, DEP_DELAY as DELAY1\n",
    "FROM c_airports as air, count_airports as c\n",
    "WHERE air.ORIGIN = c.ORIGIN AND NUM_ROW = P\n",
    "\"\"\")\n",
    "p_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|AIRPORT|DELAY2|\n",
      "+-------+------+\n",
      "|    SAN|  -7.0|\n",
      "|    PHX|  -8.0|\n",
      "|    DFW|  -7.0|\n",
      "|    MIA|  -7.0|\n",
      "|    BOS|  -7.0|\n",
      "|    BWI|     0|\n",
      "|    LAX|  -8.0|\n",
      "|    SFO|  -8.0|\n",
      "|    DEN|  -9.0|\n",
      "|    MSP|  -5.0|\n",
      "|    DTW|  -6.0|\n",
      "|    DCA|  -7.0|\n",
      "|    BNA|  -8.0|\n",
      "|    ORD|  -7.0|\n",
      "|    LAS|  -8.0|\n",
      "|    MCO|  -8.0|\n",
      "|    SLC|  -6.0|\n",
      "|    IAH|  -6.0|\n",
      "|    CLT|  -7.0|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#match to each P1 value the recording row of the delay\n",
    "p1_set = spark.sql(\"\"\"\n",
    "SELECT air.ORIGIN as AIRPORT, DEP_DELAY as DELAY2\n",
    "FROM c_airports as air, count_airports as c\n",
    "WHERE air.ORIGIN = c.ORIGIN AND NUM_ROW = P1\n",
    "\"\"\")\n",
    "p1_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_set.createOrReplaceTempView(\"p_set\")\n",
    "p1_set.createOrReplaceTempView(\"p1_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|AIRPORT|DELAY1|DELAY2|\n",
      "+-------+------+------+\n",
      "|    DCA|  -7.0|  -7.0|\n",
      "|    IAH|  -6.0|  -6.0|\n",
      "|    LGA|  -7.0|  null|\n",
      "|    BOS|  -7.0|  -7.0|\n",
      "|    EWR|  -8.0|  null|\n",
      "|    LAS|  -8.0|  -8.0|\n",
      "|    DEN|  -9.0|  -9.0|\n",
      "|    SEA|  -7.0|  null|\n",
      "|    BNA|  -8.0|  -8.0|\n",
      "|    CLT|  -7.0|  -7.0|\n",
      "|    MIA|  -7.0|  -7.0|\n",
      "|    BWI|     0|     0|\n",
      "|    TPA|  -7.0|  null|\n",
      "|    PHX|  -8.0|  -8.0|\n",
      "|    DFW|  -7.0|  -7.0|\n",
      "|    SFO|  -8.0|  -8.0|\n",
      "|    ATL|  -6.0|  null|\n",
      "|    FLL|  -8.0|  null|\n",
      "|    ORD|  -7.0|  -7.0|\n",
      "|    MDW|   1.0|  null|\n",
      "+-------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join the above tables to a final one\n",
    "med_air = spark.sql(\"\"\"\n",
    "SELECT p_set.AIRPORT, DELAY1, DELAY2\n",
    "FROM p_set\n",
    "LEFT JOIN p1_set\n",
    "ON p_set.AIRPORT = p1_set.AIRPORT\n",
    "\"\"\")\n",
    "med_air.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_air.createOrReplaceTempView(\"med_air\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|AIRPORT|MEDIAN|\n",
      "+-------+------+\n",
      "|    MDW|   1.0|\n",
      "|    BWI|   0.0|\n",
      "|    DEN|  -9.0|\n",
      "|    MCO|  -8.0|\n",
      "|    EWR|  -8.0|\n",
      "|    LAS|  -8.0|\n",
      "|    PHX|  -8.0|\n",
      "|    SFO|  -8.0|\n",
      "|    LAX|  -8.0|\n",
      "|    FLL|  -8.0|\n",
      "|    BNA|  -8.0|\n",
      "|    DCA|  -7.0|\n",
      "|    BOS|  -7.0|\n",
      "|    LGA|  -7.0|\n",
      "|    SEA|  -7.0|\n",
      "|    MIA|  -7.0|\n",
      "|    TPA|  -7.0|\n",
      "|    DFW|  -7.0|\n",
      "|    CLT|  -7.0|\n",
      "|    SAN|  -7.0|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the median and sort it\n",
    "med_air = spark.sql(\"\"\"\n",
    "SELECT AIRPORT, IF((DELAY2 IS NULL), DELAY1, (DELAY1 + DELAY2)/2) AS MEDIAN\n",
    "FROM med_air\n",
    "ORDER BY MEDIAN DESC\n",
    "\"\"\")\n",
    "med_air.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_air.toPandas().to_csv('task2-ap-med.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|Airways|Departures_Delay|\n",
      "+-------+----------------+\n",
      "|     B6|          17.528|\n",
      "|     EV|          16.451|\n",
      "|     F9|          14.346|\n",
      "|     YV|          13.412|\n",
      "|     UA|          12.898|\n",
      "|     OO|          12.313|\n",
      "|     AA|          11.868|\n",
      "|     NK|          10.756|\n",
      "|     OH|          10.468|\n",
      "|     9E|          10.085|\n",
      "|     G4|          10.065|\n",
      "|     WN|            9.93|\n",
      "|     MQ|           8.967|\n",
      "|     YX|           8.356|\n",
      "|     DL|           8.141|\n",
      "|     AS|            4.98|\n",
      "|     HA|           1.295|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we want the avg for the departures delays for every airway (carrier)\n",
    "avg_airways = spark.sql(\"\"\"\n",
    "SELECT fd.CARRIER as Airways, round(avg(fd.DEP_DELAY1), 3) as Departures_Delay\n",
    "FROM flight_data as fd, airways as a\n",
    "WHERE fd.CARRIER = a.airways\n",
    "GROUP BY fd.CARRIER\n",
    "ORDER BY avg(fd.DEP_DELAY1) DESC\"\"\")\n",
    "avg_airways.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_airways.toPandas().to_csv('task2-aw-avg.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+\n",
      "|NUM_ROW|CARRIER|DEP_DELAY|\n",
      "+-------+-------+---------+\n",
      "|      1|     UA|     -1.0|\n",
      "|      2|     UA|     -1.0|\n",
      "|      3|     UA|     -1.0|\n",
      "|      4|     UA|     -1.0|\n",
      "|      5|     UA|     -1.0|\n",
      "|      6|     UA|     -1.0|\n",
      "|      7|     UA|     -1.0|\n",
      "|      8|     UA|     -1.0|\n",
      "|      9|     UA|     -1.0|\n",
      "|     10|     UA|     -1.0|\n",
      "|     11|     UA|     -1.0|\n",
      "|     12|     UA|     -1.0|\n",
      "|     13|     UA|     -1.0|\n",
      "|     14|     UA|     -1.0|\n",
      "|     15|     UA|     -1.0|\n",
      "|     16|     UA|     -1.0|\n",
      "|     17|     UA|     -1.0|\n",
      "|     18|     UA|     -1.0|\n",
      "|     19|     UA|     -1.0|\n",
      "|     20|     UA|     -1.0|\n",
      "+-------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The same steps as the \"Report 2\"\n",
    "c_airways = spark.sql(\"\"\"\n",
    "SELECT ROW_NUMBER() OVER(PARTITION BY CARRIER \n",
    "                            ORDER BY DEP_DELAY1 ASC) AS NUM_ROW,\n",
    "        CARRIER, DEP_DELAY1 AS DEP_DELAY\n",
    "FROM flight_data, airways as a\n",
    "WHERE CARRIER = a.airways \n",
    "\"\"\")\n",
    "c_airways.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|CARRIER|    CAR|\n",
      "+-------+-------+\n",
      "|     UA| 625910|\n",
      "|     NK| 204845|\n",
      "|     AA| 946776|\n",
      "|     EV| 134683|\n",
      "|     B6| 297411|\n",
      "|     DL| 991986|\n",
      "|     OO| 836445|\n",
      "|     F9| 135543|\n",
      "|     YV| 227888|\n",
      "|     MQ| 327007|\n",
      "|     OH| 289304|\n",
      "|     HA|  83891|\n",
      "|     G4| 105305|\n",
      "|     YX| 329149|\n",
      "|     AS| 264816|\n",
      "|     WN|1363946|\n",
      "|     9E| 257132|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_airways = spark.sql(\"\"\"\n",
    "SELECT CARRIER, count(CARRIER) as CAR\n",
    "FROM flight_data, airways as a\n",
    "WHERE CARRIER = a.airways\n",
    "GROUP BY CARRIER\n",
    "\"\"\")\n",
    "count_airways.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_airways.createOrReplaceTempView(\"count_airways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+------+\n",
      "|CARRIER|    CAR|     P|    P1|\n",
      "+-------+-------+------+------+\n",
      "|     UA| 625910|312955|312956|\n",
      "|     NK| 204845|102423|     0|\n",
      "|     AA| 946776|473388|473389|\n",
      "|     EV| 134683| 67342|     0|\n",
      "|     B6| 297411|148706|     0|\n",
      "|     DL| 991986|495993|495994|\n",
      "|     OO| 836445|418223|     0|\n",
      "|     F9| 135543| 67772|     0|\n",
      "|     YV| 227888|113944|113945|\n",
      "|     MQ| 327007|163504|     0|\n",
      "|     OH| 289304|144652|144653|\n",
      "|     HA|  83891| 41946|     0|\n",
      "|     G4| 105305| 52653|     0|\n",
      "|     YX| 329149|164575|     0|\n",
      "|     AS| 264816|132408|132409|\n",
      "|     WN|1363946|681973|681974|\n",
      "|     9E| 257132|128566|128567|\n",
      "+-------+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_airways = spark.sql(\"\"\"\n",
    "SELECT CARRIER, CAR, CAST(IF(MOD(CAR, 2)=0, CAR/2, (CAR+1)/2) AS int) AS P,\n",
    "        CAST(IF(MOD(CAR, 2)=0, CAR/2+1, 0) AS int) AS P1\n",
    "FROM count_airways\n",
    "\"\"\")\n",
    "count_airways.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_airways.createOrReplaceTempView(\"c_airways\")\n",
    "count_airways.createOrReplaceTempView(\"count_airways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|AIRWAY|DELAY1|\n",
      "+------+------+\n",
      "|    EV|  -7.0|\n",
      "|    UA|  -7.0|\n",
      "|    NK|  -6.0|\n",
      "|    9E|  -6.0|\n",
      "|    G4|  -8.0|\n",
      "|    HA|  -7.0|\n",
      "|    YX|  -6.0|\n",
      "|    WN|   0.0|\n",
      "|    AS|  -7.0|\n",
      "|    YV|  -7.0|\n",
      "|    B6|  -8.0|\n",
      "|    MQ|  -6.0|\n",
      "|    OO|  -6.0|\n",
      "|    OH|  -7.0|\n",
      "|    F9|  -8.0|\n",
      "|    DL|  -6.0|\n",
      "|    AA|  -7.0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_set = spark.sql(\"\"\"\n",
    "SELECT air.CARRIER as AIRWAY, DEP_DELAY as DELAY1\n",
    "FROM c_airways as air, count_airways as c\n",
    "WHERE air.CARRIER = c.CARRIER AND NUM_ROW = P\n",
    "\"\"\")\n",
    "p_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|AIRWAY|DELAY2|\n",
      "+------+------+\n",
      "|    WN|   0.0|\n",
      "|    9E|  -6.0|\n",
      "|    DL|  -6.0|\n",
      "|    YV|  -7.0|\n",
      "|    AS|  -7.0|\n",
      "|    OH|  -7.0|\n",
      "|    UA|  -7.0|\n",
      "|    AA|  -7.0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p1_set = spark.sql(\"\"\"\n",
    "SELECT air.CARRIER as AIRWAY, DEP_DELAY as DELAY2\n",
    "FROM c_airways as air, count_airways as c\n",
    "WHERE air.CARRIER = c.CARRIER AND NUM_ROW = P1\n",
    "\"\"\")\n",
    "p1_set.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_set.createOrReplaceTempView(\"p_set\")\n",
    "p1_set.createOrReplaceTempView(\"p1_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|AIRWAY|DELAY1|DELAY2|\n",
      "+------+------+------+\n",
      "|    UA|  -7.0|  -7.0|\n",
      "|    NK|  -6.0|  null|\n",
      "|    AA|  -7.0|  -7.0|\n",
      "|    EV|  -7.0|  null|\n",
      "|    B6|  -8.0|  null|\n",
      "|    DL|  -6.0|  -6.0|\n",
      "|    OO|  -6.0|  null|\n",
      "|    F9|  -8.0|  null|\n",
      "|    YV|  -7.0|  -7.0|\n",
      "|    MQ|  -6.0|  null|\n",
      "|    OH|  -7.0|  -7.0|\n",
      "|    HA|  -7.0|  null|\n",
      "|    G4|  -8.0|  null|\n",
      "|    YX|  -6.0|  null|\n",
      "|    AS|  -7.0|  -7.0|\n",
      "|    WN|   0.0|   0.0|\n",
      "|    9E|  -6.0|  -6.0|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "med_air = spark.sql(\"\"\"\n",
    "SELECT p_set.AIRWAY, DELAY1, DELAY2\n",
    "FROM p_set\n",
    "LEFT JOIN p1_set\n",
    "ON p_set.AIRWAY = p1_set.AIRWAY\n",
    "\"\"\")\n",
    "med_air.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_air.createOrReplaceTempView(\"med_air\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|AIRWAY|MEDIAN|\n",
      "+------+------+\n",
      "|    WN|   0.0|\n",
      "|    B6|  -8.0|\n",
      "|    G4|  -8.0|\n",
      "|    F9|  -8.0|\n",
      "|    UA|  -7.0|\n",
      "|    AA|  -7.0|\n",
      "|    EV|  -7.0|\n",
      "|    YV|  -7.0|\n",
      "|    OH|  -7.0|\n",
      "|    HA|  -7.0|\n",
      "|    AS|  -7.0|\n",
      "|    NK|  -6.0|\n",
      "|    MQ|  -6.0|\n",
      "|    DL|  -6.0|\n",
      "|    9E|  -6.0|\n",
      "|    OO|  -6.0|\n",
      "|    YX|  -6.0|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "med_air = spark.sql(\"\"\"\n",
    "SELECT AIRWAY, IF((DELAY2 IS NULL), DELAY1, (DELAY1 + DELAY2)/2) AS MEDIAN\n",
    "FROM med_air\n",
    "ORDER BY MEDIAN DESC\n",
    "\"\"\")\n",
    "med_air.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_air.toPandas().to_csv('task2-aw-med.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+-----+\n",
      "|airports|airways|dep_time|delay|\n",
      "+--------+-------+--------+-----+\n",
      "|     DCA|     UA|    1602| -3.0|\n",
      "|     DCA|     UA|     821| -9.0|\n",
      "|     DCA|     UA|    1711| 26.0|\n",
      "|     DCA|     UA|    1649| 24.0|\n",
      "|     DCA|     UA|    1144| -1.0|\n",
      "|     DCA|     UA|     602| -8.0|\n",
      "|     DCA|     UA|     737| -8.0|\n",
      "|     DCA|     UA|     838| -7.0|\n",
      "|     DCA|     UA|    1559| -6.0|\n",
      "|     DCA|     UA|     828| -2.0|\n",
      "|     DCA|     UA|    1640| -5.0|\n",
      "|     DCA|     UA|    1634|  9.0|\n",
      "|     DCA|     UA|     648| -7.0|\n",
      "|     DCA|     UA|     625| 15.0|\n",
      "|     DCA|     UA|     740| -5.0|\n",
      "|     DCA|     UA|    1836|151.0|\n",
      "|     DCA|     UA|     820|-10.0|\n",
      "|     DCA|     UA|    1656| 11.0|\n",
      "|     DCA|     UA|    1649| 24.0|\n",
      "|     DCA|     UA|     644|-11.0|\n",
      "+--------+-------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#combine the above tables to one\n",
    "data = spark.sql(\"\"\"\n",
    "SELECT ORIGIN AS airports, CARRIER AS airways, DEP_TIME AS dep_time, DEP_DELAY1 AS delay\n",
    "FROM flight_data, flights, airways\n",
    "WHERE CARRIER = airways AND ORIGIN = airports\n",
    "\"\"\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+-----+\n",
      "|airports|airways|dep_time|delay|\n",
      "+--------+-------+--------+-----+\n",
      "|     DCA|     UA|      16| -3.0|\n",
      "|     DCA|     UA|       8| -9.0|\n",
      "|     DCA|     UA|      17| 26.0|\n",
      "|     DCA|     UA|      16| 24.0|\n",
      "|     DCA|     UA|      11| -1.0|\n",
      "|     DCA|     UA|       6| -8.0|\n",
      "|     DCA|     UA|       7| -8.0|\n",
      "|     DCA|     UA|       8| -7.0|\n",
      "|     DCA|     UA|      15| -6.0|\n",
      "|     DCA|     UA|       8| -2.0|\n",
      "|     DCA|     UA|      16| -5.0|\n",
      "|     DCA|     UA|      16|  9.0|\n",
      "|     DCA|     UA|       6| -7.0|\n",
      "|     DCA|     UA|       6| 15.0|\n",
      "|     DCA|     UA|       7| -5.0|\n",
      "|     DCA|     UA|      18|151.0|\n",
      "|     DCA|     UA|       8|-10.0|\n",
      "|     DCA|     UA|      16| 11.0|\n",
      "|     DCA|     UA|      16| 24.0|\n",
      "|     DCA|     UA|       6|-11.0|\n",
      "+--------+-------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#place 0 when the time had 3 digits and select only the hour\n",
    "data = spark.sql(\"\"\"\n",
    "SELECT airports,\n",
    "airways, CAST(SUBSTRING(IF(0<(dep_time DIV 1000) AND (dep_time DIV 1000)<=9, dep_time, CONCAT(0,dep_time)),1,2) AS int) AS dep_time,\n",
    "delay\n",
    "FROM data\n",
    "\"\"\")\n",
    "data.show()\n",
    "data.createOrReplaceTempView(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|dep_time|\n",
      "+--------+\n",
      "|       0|\n",
      "|       1|\n",
      "|       2|\n",
      "|       3|\n",
      "|       4|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create the \"df\" for the one-hot encoding\n",
    "df = data.na.fill(0)\n",
    "\n",
    "df.select(\"dep_time\").distinct().orderBy(\"dep_time\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One - Hot - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+-----+----------------+---------------+----------------+------------------------+-----------------------+------------------------+--------------------+\n",
      "|airports|airways|dep_time|delay|airports_indexed|airways_indexed|dep_time_indexed|airports_indexed_encoded|airways_indexed_encoded|dep_time_indexed_encoded|            features|\n",
      "+--------+-------+--------+-----+----------------+---------------+----------------+------------------------+-----------------------+------------------------+--------------------+\n",
      "|     DCA|     UA|      16| -3.0|            16.0|            3.0|             9.0|         (28,[16],[1.0])|         (17,[3],[1.0])|          (25,[9],[1.0])|(70,[16,31,54],[1...|\n",
      "|     DCA|     UA|       8| -9.0|            16.0|            3.0|             0.0|         (28,[16],[1.0])|         (17,[3],[1.0])|          (25,[0],[1.0])|(70,[16,31,45],[1...|\n",
      "|     DCA|     UA|      17| 26.0|            16.0|            3.0|             7.0|         (28,[16],[1.0])|         (17,[3],[1.0])|          (25,[7],[1.0])|(70,[16,31,52],[1...|\n",
      "|     DCA|     UA|      16| 24.0|            16.0|            3.0|             9.0|         (28,[16],[1.0])|         (17,[3],[1.0])|          (25,[9],[1.0])|(70,[16,31,54],[1...|\n",
      "|     DCA|     UA|      11| -1.0|            16.0|            3.0|             5.0|         (28,[16],[1.0])|         (17,[3],[1.0])|          (25,[5],[1.0])|(70,[16,31,50],[1...|\n",
      "+--------+-------+--------+-----+----------------+---------------+----------------+------------------------+-----------------------+------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding of all the string columns\n",
    "categorical_columns= ['airports', 'airways', 'dep_time']\n",
    "\n",
    "# The index of string vlaues multiple columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n",
    "    for c in categorical_columns\n",
    "]\n",
    "\n",
    "# The encode of indexed vlaues multiple columns\n",
    "encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n",
    "            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n",
    "    for indexer in indexers\n",
    "]\n",
    "\n",
    "# Vectorizing encoded values\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders+[assembler])\n",
    "model=pipeline.fit(df)\n",
    "newdf = model.transform(df)\n",
    "newdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airports: string (nullable = true)\n",
      " |-- airways: string (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- delay: string (nullable = false)\n",
      " |-- airports_indexed: double (nullable = false)\n",
      " |-- airways_indexed: double (nullable = false)\n",
      " |-- dep_time_indexed: double (nullable = false)\n",
      " |-- airports_indexed_encoded: vector (nullable = true)\n",
      " |-- airways_indexed_encoded: vector (nullable = true)\n",
      " |-- dep_time_indexed_encoded: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4632951"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training - Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+--------------------+-----+\n",
      "|airports|airways|dep_time|            features|delay|\n",
      "+--------+-------+--------+--------------------+-----+\n",
      "|     DCA|     UA|      16|(70,[16,31,54],[1...| -3.0|\n",
      "|     DCA|     UA|       8|(70,[16,31,45],[1...| -9.0|\n",
      "|     DCA|     UA|      17|(70,[16,31,52],[1...| 26.0|\n",
      "|     DCA|     UA|      16|(70,[16,31,54],[1...| 24.0|\n",
      "|     DCA|     UA|      11|(70,[16,31,50],[1...| -1.0|\n",
      "+--------+-------+--------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we are going to need features and delay columns\n",
    "lrdata = newdf.select('airports', 'airways', 'dep_time','features', newdf.delay.cast(\"float\"))\n",
    "lrdata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 3244238\n",
      "Size of test set: 1388713\n"
     ]
    }
   ],
   "source": [
    "#split to training and test sets\n",
    "training, test = lrdata.randomSplit(weights = [0.70, 0.30], seed = 1)\n",
    "print(\"Size of training set: \" + str(training.count()))\n",
    "print(\"Size of test set: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-2.5095520308126877,3.2798728865289153,0.7703989217405849,0.17208236459496357,-0.7916723847024338,-1.8631365549360148,0.0,-1.9779656987909404,3.4422780018382517,0.0,-2.1986825471654723,0.0,0.0,1.4679644021154479,0.5089598092216664,-1.8177257233461326,0.0,5.573663496164053,0.0,0.0,-3.38710535109035,0.0,0.0,-0.7361105636361487,0.0,0.0,0.0,0.0,0.0,-1.4463920074517074,0.0,0.0,0.0,2.77317160891493,-3.15606104220751,-4.798075508572511,-2.0310058858863123,-1.062197305559125,-0.14886754164384464,0.0,1.503232764240975,0.0,0.0,0.0,0.0,-6.643712988665897,-4.47551993337573,5.028108777226659,0.0,-2.682604107407095,-1.6208124642905375,-7.636206359192928,2.430600216768728,-0.6063945873935096,1.6125604775000009,0.0,3.964418970873359,5.216521680387868,0.0,-9.221566746189062,11.742318207199745,12.133337497713047,-8.896448176575875,-9.599529385910342,37.931061818522366,75.9804466787146,10.2598878586563,88.16745716551331,68.7302414737115,47.65280331904116]\n",
      "Intercept: 11.248573553586644\n"
     ]
    }
   ],
   "source": [
    "#create the linear regression model\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='delay', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(training)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|             delay|          dep_time|\n",
      "+-------+------------------+------------------+\n",
      "|  count|           3244238|           3244238|\n",
      "|   mean|11.538565912858427|13.313012177281692|\n",
      "| stddev|46.530599534866546|5.3244586509628915|\n",
      "|    min|             -59.0|                 0|\n",
      "|    max|            2672.0|                24|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#summary and goodness of fit\n",
    "training.select('delay','dep_time').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on training data = 0.052316\n",
      "Root Mean Squared Error (RMSE) on training data = 45.297093\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"R Squared (R2) on training data = %f\" % trainingSummary.r2)\n",
    "print(\"Root Mean Squared Error (RMSE) on training data = %f\" % trainingSummary.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predictions with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+-------+--------+\n",
      "|          prediction|delay|airports|airways|dep_time|\n",
      "+--------------------+-----+--------+-------+--------+\n",
      "| -0.8605078631363856|  0.0|     ATL|     UA|       0|\n",
      "|   96.90647868828727|403.0|     ATL|     UA|       2|\n",
      "|   77.46926299648545|242.0|     ATL|     UA|       3|\n",
      "|   77.46926299648545|243.0|     ATL|     UA|       3|\n",
      "|   77.46926299648545|273.0|     ATL|     UA|       3|\n",
      "|-0.15742665380191845| -8.0|     ATL|     UA|       5|\n",
      "|-0.15742665380191845| -6.0|     ATL|     UA|       5|\n",
      "|-0.15742665380191845| -4.0|     ATL|     UA|       5|\n",
      "|-0.15742665380191845| -3.0|     ATL|     UA|       5|\n",
      "|-0.15742665380191845| -2.0|     ATL|     UA|       5|\n",
      "+--------------------+-----+--------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test)\n",
    "lr_predictions.select(\"prediction\",\"delay\",'airports', 'airways', 'dep_time').distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.0532136\n",
      "Root Mean Squared Error (RMSE) on test data = 45.3351\n"
     ]
    }
   ],
   "source": [
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"delay\",metricName=\"r2\")\n",
    "test_eval = lr_model.evaluate(test)\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_eval.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|             delay|          dep_time|\n",
      "+-------+------------------+------------------+\n",
      "|  count|           1388713|           1388713|\n",
      "|   mean|11.551303257044472|13.322641179278945|\n",
      "| stddev|46.591704364518684| 5.320371820639896|\n",
      "|    min|             -48.0|                 0|\n",
      "|    max|            1790.0|                24|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.select('delay','dep_time').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
